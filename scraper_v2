# navigate to folder
# run: source venv/bin/activate
# save dependencies: pip freeze > requirements.txt


import requests
import warnings
import csv
from bs4 import BeautifulSoup
from datetime import datetime


def get_webpage_content(url):
    response = requests.get(url)
    if response.status_code == 200:
        return response.content
    else:
        print(f"Failed to retrieve page. Status code: {response.status_code}")
        return None


def extract_filtered_links(content, base_url):
    soup = BeautifulSoup(content, 'html.parser')
    links = soup.find_all('a')
    
    # Filter and clean the links
    unique_filtered_links = set()
    for link in links:
        if link.has_attr('href') and link['href'].startswith(base_url):
            clean_link = link['href'].split('#')[0]  # Remove the #-part
            unique_filtered_links.add(clean_link)

    return unique_filtered_links



def recipe_extractor(link):
    content = get_webpage_content(link)
    if content and 'class="wprm-recipe-ingredients"' in content.decode('utf-8'):
    
        soup = BeautifulSoup(content, 'html.parser')
        
        # Extract title
        title_tag = soup.find('h1', class_='entry-title')
        title = title_tag.get_text(strip=True) if title_tag else None
        
        # Extract image URL
        image_url = None
        if title_tag:  # Ensure the title tag is present
            subsequent_images = title_tag.find_all_next('img', limit=1)
            if subsequent_images:
                image_url = subsequent_images[0].get('src')
        
        # Construct the data dictionary
        if image_url:
            recipe_data = {
                'title': title,
                'recipeUrl': link,
                'imageUrl': image_url,
                'description': '',
                'blogName': blog,
                'hidePicture': False,
                'activate': True,
                'characteristics': ''
            }
            return recipe_data
        else:
            return False
        
    else:
        return False



# !! START HERE !!
base_url = 'https://elavegan.com/de/'
content = get_webpage_content(base_url)

all_links = set()
all_recipes = []

path = 'extracts/'
now = datetime.today().strftime("%Y-%m-%d-%H%M%S")
blog = base_url.split('//')[1].split('/')[0]
filename = path + now + '_' +  blog + '.csv'



with warnings.catch_warnings():
    warnings.simplefilter("ignore", category=UserWarning)

# Extract 1st level links
    if content:
        filtered_links = extract_filtered_links(content, base_url)
        all_links.update(filtered_links)
        for link in all_links:
            recipe_data = recipe_extractor(link) 
            if recipe_data:  # Check if the extractor returned data
                all_recipes.append(recipe_data)
        

    with open(filename, 'w', newline='') as csvfile:
        fieldnames = ['title', 'recipeUrl', 'imageUrl', 'description', 'blogName', 'hidePicture', 'activate', 'characteristics']  # the keys in your dictionaries
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader() 
        for recipe in all_recipes:
            writer.writerow(recipe)

print('done')

    # Extract 2nd level links
        # for link in filtered_links:
        #     content = get_webpage_content(link)
        #     if content:
        #         try:
        #             filtered_links_l2 = extract_filtered_links(content, base_url)
        #             all_links.update(filtered_links_l2)
        #         except Exception as e:
        #                 print(f"Error parsing link {link}: {e}")

    # print(all_links)
    # print(len(all_links))            



    # for link in filtered_links:
    #     if is_recipe_page(link):
    #         recipe_data = extract_recipe_details(link)
    #         if recipe_data:
    #             print(recipe_data)

